<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="assets/audio_logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-Audio <a href="https://www.modelscope.cn/models/qwen/QWen-Audio/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-Audio">ğŸ¤—</a>&nbsp ï½œ Qwen-Audio-Chat <a href="https://www.modelscope.cn/models/qwen/QWen-Audio-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-Audio-Chat">ğŸ¤—</a>&nbsp | &nbsp&nbsp Demo<a href="https://modelscope.cn/studios/qwen/Qwen-Audio-Chat-Demo/summary"> ğŸ¤–</a> | <a href="https://huggingface.co/spaces/Qwen/Qwen-Audio">ğŸ¤—</a>&nbsp
<br>
&nbsp&nbsp<a href="https://qwen-audio.github.io/Qwen-Audio/">Homepage</a>&nbsp ï½œ &nbsp&nbsp<a href="http://arxiv.org/abs/2311.07919">Paper</a>&nbsp&nbsp | &nbsp&nbsp&nbsp<a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp</a>
</p>
<br><br>

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/speech-recognition-on-aishell-1)](https://paperswithcode.com/sota/speech-recognition-on-aishell-1?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/speech-recognition-on-aishell-2-test-android-1)](https://paperswithcode.com/sota/speech-recognition-on-aishell-2-test-android-1?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/speech-recognition-on-aishell-2-test-ios)](https://paperswithcode.com/sota/speech-recognition-on-aishell-2-test-ios?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/speech-recognition-on-aishell-2-test-mic-1)](https://paperswithcode.com/sota/speech-recognition-on-aishell-2-test-mic-1?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/acoustic-scene-classification-on-cochlscene)](https://paperswithcode.com/sota/acoustic-scene-classification-on-cochlscene?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/acoustic-scene-classification-on-tut-acoustic)](https://paperswithcode.com/sota/acoustic-scene-classification-on-tut-acoustic?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/audio-classification-on-vocalsound)](https://paperswithcode.com/sota/audio-classification-on-vocalsound?p=qwen-audio-advancing-universal-audio) <br>
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/audio-captioning-on-clotho)](https://paperswithcode.com/sota/audio-captioning-on-clotho?p=qwen-audio-advancing-universal-audio) <br>
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/speech-recognition-on-librispeech-test-clean)](https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/emotion-recognition-in-conversation-on-meld)](https://paperswithcode.com/sota/emotion-recognition-in-conversation-on-meld?p=qwen-audio-advancing-universal-audio)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/qwen-audio-advancing-universal-audio/speech-recognition-on-librispeech-test-other)](https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-other?p=qwen-audio-advancing-universal-audio)

**Qwen-Audio** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLarge Audio Language Modelï¼‰ã€‚Qwen-Audio å¯ä»¥ä»¥å¤šç§éŸ³é¢‘ (åŒ…æ‹¬è¯´è¯äººè¯­éŸ³ã€è‡ªç„¶éŸ³ã€éŸ³ä¹ã€æ­Œå£°ï¼‰å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚Qwen-Audio ç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š

- **éŸ³é¢‘åŸºçŸ³æ¨¡å‹**ï¼šQwen-Audioæ˜¯ä¸€ä¸ªæ€§èƒ½å“è¶Šçš„é€šç”¨çš„éŸ³é¢‘ç†è§£æ¨¡å‹ï¼Œæ”¯æŒå„ç§ä»»åŠ¡ã€è¯­è¨€å’ŒéŸ³é¢‘ç±»å‹ã€‚åœ¨Qwen-Audioçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡ä»¤å¾®è°ƒå¼€å‘äº†Qwen-Audio-Chatï¼Œæ”¯æŒå¤šè½®ã€å¤šè¯­è¨€ã€å¤šè¯­è¨€å¯¹è¯ã€‚Qwen-Audioå’ŒQwen-Audio-Chatæ¨¡å‹å‡å·²å¼€æºã€‚
- **å…¼å®¹å¤šç§å¤æ‚éŸ³é¢‘çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶**ï¼šä¸ºäº†é¿å…ç”±äºæ•°æ®æ”¶é›†æ¥æºä¸åŒä»¥åŠä»»åŠ¡ç±»å‹ä¸åŒï¼Œå¸¦æ¥çš„éŸ³é¢‘åˆ°æ–‡æœ¬çš„ä¸€å¯¹å¤šçš„å¹²æ‰°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡è®­ç»ƒæ¡†æ¶ï¼Œå®ç°ç›¸ä¼¼ä»»åŠ¡çš„çŸ¥è¯†å…±äº«ï¼Œå¹¶å°½å¯èƒ½å‡å°‘ä¸åŒä»»åŠ¡ä¹‹é—´çš„å¹²æ‰°ã€‚é€šè¿‡æå‡ºçš„æ¡†æ¶ï¼ŒQwen-Audioå¯ä»¥å®¹çº³è®­ç»ƒè¶…è¿‡30å¤šç§ä¸åŒçš„éŸ³é¢‘ä»»åŠ¡ï¼›
- **å‡ºè‰²çš„æ€§èƒ½**ï¼šQwen-Audioåœ¨ä¸éœ€è¦ä»»ä½•ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å„ç§åŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœã€‚å…·ä½“å¾—ï¼ŒQwen-Audioåœ¨Aishell1ã€cochlsceneã€ClothoAQAå’ŒVocalSoundçš„æµ‹è¯•é›†ä¸Šéƒ½è¾¾åˆ°äº†SOTAï¼›
- **æ”¯æŒå¤šè½®éŸ³é¢‘å’Œæ–‡æœ¬å¯¹è¯ï¼Œæ”¯æŒå„ç§è¯­éŸ³åœºæ™¯**ï¼šQwen-Audio-Chatæ”¯æŒå£°éŸ³ç†è§£å’Œæ¨ç†ã€éŸ³ä¹æ¬£èµã€å¤šéŸ³é¢‘åˆ†æã€å¤šè½®éŸ³é¢‘-æ–‡æœ¬äº¤é”™å¯¹è¯ä»¥åŠå¤–éƒ¨è¯­éŸ³å·¥å…·çš„ä½¿ç”¨ã€‚

<br>
<p align="center">
    <img src="assets/framework.png" width="800"/>
<p>
<br>


æˆ‘ä»¬æä¾›äº† Qwen-Audio ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼š
- Qwen-Audio: Qwen-Audio ä»¥ [Qwen-7B](https://github.com/QwenLM/Qwen) çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºè¯­è¨€æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶ä»¥ [Whisper-large-v2](https://github.com/openai/whisper) ä½œä¸ºéŸ³é¢‘ç¼–ç å™¨çš„åˆå§‹åŒ–ã€‚
- Qwen-Audio-Chat: åœ¨ Qwen-Audio çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯­éŸ³AIåŠ©æ‰‹Qwen-Audio-Chatï¼Œå®ƒæ”¯æŒæ›´çµæ´»çš„äº¤äº’æ–¹å¼ï¼ŒåŒ…æ‹¬å¤šéŸ³é¢‘ã€å¤šè½®é—®ç­”ã€åˆ›ä½œç­‰èƒ½åŠ›ã€‚
  <br>

## æ–°é—»
* 2023.11.30 ğŸ”¥ **Qwen-Audio**å’Œ**Qwen-Audio-Chat**çš„æ¨¡å‹æƒé‡å·²ç»åœ¨Hugging Faceå’ŒModelScopeå¼€æºã€‚
* 2023.11.15 ğŸ‰ æˆ‘ä»¬å‘å¸ƒäº†Qwen-Audioç³»åˆ—æ¨¡å‹çš„[è®ºæ–‡](http://arxiv.org/abs/2311.07919), ä»‹ç»äº†ç›¸å…³çš„æ¨¡å‹ç»“æ„ï¼Œè®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹è¡¨ç°ã€‚
<br>

## è¯„æµ‹
æˆ‘ä»¬åœ¨æ ‡å‡†çš„12ä¸ªå­¦æœ¯æ•°æ®é›†ä¸Šè¯„æµ‹äº†æ¨¡å‹çš„èƒ½åŠ›

<p align="center">
    <img src="assets/evaluation.png" width="800"/>
<p>


ç»¼åˆè¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š
<p align="center">
    <img src="assets/radar_new.png" width="800"/>
<p>


å„é¡¹æŒ‡æ ‡ç»†èŠ‚å¦‚ä¸‹:
### ä¸­è‹±æ–‡è¯­éŸ³è¯†åˆ«ï¼ˆAutomatic Speech Recognitionï¼‰
è‹±æ–‡è¯­éŸ³è¯†åˆ«
<table>
<thead>
<tr>
    <th rowspan="2">Dataset</th>
    <th rowspan="2">Model</th>
    <th colspan="4">Results (WER)</th>
  </tr>
<tr>
    <th>dev-clean</th>
    <th>dev-othoer</th>
    <th>test-clean</th>
    <th>test-other</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="5">Librispeech</td>
    <td>SpeechT5</td>
    <td>2.1</td>
    <td>5.5</td>
    <td>2.4</td>
    <td>5.8</td>
  </tr>
  <tr>
    <td>SpeechNet</td>
    <td>-</td>
    <td>-</td>
    <td>30.7</td>
    <td>-</td>
  </tr>
<tr>
    <td>SLM-FT</td>
    <td>-</td>
    <td>-</td>
    <td>2.6</td>
    <td>5.0</td>
  </tr>
<tr>
    <td>SALMONN</td>
    <td>-</td>
    <td>-</td>
    <td>2.1</td>
    <td>4.9</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>1.8</strong></td>
    <td><strong>4.0</strong></td>
    <td><strong>2.0</strong></td>
    <td><strong>4.2</strong></td>
  </tr>
</table>

ä¸­æ–‡è¯­éŸ³è¯†åˆ«

<table>
<thead>
<tr>
    <th rowspan="2">Dataset</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Results (WER)</th>
  </tr>
<tr>
    <th>dev</th>
    <th>test</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="4">Aishell1</td>
    <td>MMSpeech-base</td>
    <td>2.0</td>
    <td>2.1</td>
  </tr>
<tr>
    <td>MMSpeech-large</td>
    <td>1.6</td>
    <td>1.9</td>
  </tr>
<tr>
    <td>Paraformer-large</td>
    <td>-</td>
    <td>2.0</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>1.2 (SOTA)</strong></td>
    <td><strong>1.3 (SOTA)</strong></td>
  </tr>
</table>


<table>
<thead>
<tr>
    <th rowspan="2">Dataset</th>
    <th rowspan="2">Model</th>
    <th colspan="3">Results (WER)</th>
  </tr>
<tr>
    <th>Mic</th>
    <th>iOS</th>
    <th>Android</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="3">Aishell2</td>
    <td>MMSpeech-base</td>
    <td>4.5</td>
    <td>3.9</td>
    <td>4.0</td>
  </tr>
<tr>
    <td>Paraformer-large</td>
    <td>-</td>
    <td><strong>2.9</strong></td>
    <td>-</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>3.3</strong></td>
    <td>3.1</td>
    <td><strong>3.3</strong></td>
  </tr>
</table>

### è¯­éŸ³ç¿»è¯‘ï¼ˆSoeech-to-text Translationï¼‰
<table>
<thead>
<tr>
    <th rowspan="2">Dataset</th>
    <th rowspan="2">Model</th>
    <th colspan="7">Results ï¼ˆBLUE)</th>
  </tr>
<tr>
    <th>en-de</th>
    <th>de-en</th>
    <th>en-zh</th>
    <th>zh-en</th>
    <th>es-en</th>
    <th>fr-en</th>
    <th>it-en</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="4">CoVoST2</td>
    <td>SALMMON</td>
    <td>18.6</td>
    <td>-</td>
    <td>33.1</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
<tr>
    <td>SpeechLLaMA</td>
    <td>-</td>
    <td>27.1</td>
    <td>-</td>
    <td>12.3</td>
    <td>27.9</td>
    <td>25.2</td>
    <td>25.9</td>
  </tr>
<tr>
    <td>BLSP</td>
    <td>14.1</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>25.1</strong></td>
    <td><strong>33.9</strong></td>
    <td><strong>41.5</strong></td>
    <td><strong>15.7</strong></td>
    <td><strong>39.7</strong></td>
    <td><strong>38.5</strong></td>
    <td><strong>36.0</strong></td>
  </tr>
</table>

### è¯­éŸ³æ ‡é¢˜ç”Ÿæˆï¼ˆAutomatic Audio Captionï¼‰
Clotho

<table>
<thead>
<tr>
    <th rowspan="2">Dataset</th>
    <th rowspan="2">Model</th>
    <th colspan="3">Results</th>
  </tr>
<tr>
    <th>CIDER</th>
    <th>SPICE</th>
    <th>SPIDEr</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="2">Clotho</td>
    <td>Pengi</td>
    <td>0.416</td>
    <td>0.126</td>
    <td>0.271</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.441</strong></td>
    <td><strong>0.136</strong></td>
    <td><strong>0.288</strong></td>
  </tr>
</table>


### å¸¦è¯çº§åˆ«æ—¶é—´æˆ³çš„è¯­éŸ³è¯†åˆ«ï¼ˆSpeech Recognition with Word-level Timestampï¼‰
<table>
<thead>
<tr>
    <th rowspan="1">Dataset</th>
    <th rowspan="1">Model</th>
    <th colspan="1">AAC (ms)</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="3">Industrial Data</td>
    <td>Force-aligner</td>
    <td>60.3</td>
  </tr>
<tr>
    <td>Paraformer-large-TP</td>
    <td>65.3</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>51.5 (SOTA)</strong></td>
  </tr>
</table>


### éŸ³é¢‘åœºæ™¯åˆ†ç±»ï¼ˆAutomatic Scene Classificationï¼‰
<table>
<thead>
<tr>
    <th rowspan="1">Dataset</th>
    <th rowspan="1">Model</th>
    <th colspan="1">ACC</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="2">Cochlscene</td>
    <td>Cochlscene</td>
    <td>0.669</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.795 (SOTA)</strong></td>
  </tr>
<tr>
    <td rowspan="2">TUT2017</td>
    <td>Pengi</td>
    <td>0.353</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.649</strong></td>
  </tr>
</table>


### è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSpeech Emotion Recognitionï¼‰
<table>
<thead>
<tr>
    <th rowspan="1">Dataset</th>
    <th rowspan="1">Model</th>
    <th colspan="1">ACC</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="2">Meld</td>
    <td>WavLM-large</td>
    <td>0.542</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.557</strong></td>
  </tr>
</table>


### åŸºäºéŸ³é¢‘çš„é—®ç­”ï¼ˆAudio Question & Answerï¼‰
ClothoAQA

<table>
<thead>
<tr>
    <th rowspan="2">Dataset</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Results</th>
  </tr>
<tr>
    <th>ACC</th>
    <th>ACC (binary)</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="3">ClothoAQA</td>
    <td>ClothoAQA</td>
    <td>0.542</td>
    <td>0.627</td>
  </tr>
<tr>
    <td>Pengi</td>
    <td>-</td>
    <td>0.645</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.579</strong></td>
    <td><strong>0.749</strong></td>
  </tr>
</table>

### è¯­éŸ³åˆ†ç±»ï¼ˆVocal Sound Classificationï¼‰

<table>
<thead>
<tr>
    <th rowspan="1">Dataset</th>
    <th rowspan="1">Model</th>
    <th colspan="1">ACC</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="3">VocalSound</td>
    <td>CLAP</td>
    <td>0.4945</td>
  </tr>
<tr>
    <td>Pengi</td>
    <td>0.6035</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.9289 (SOTA)</strong></td>
  </tr>
</table>


### éŸ³ç¬¦åˆ†æï¼ˆMusic Note Analysisï¼‰
<table>
<thead>
<tr>
    <th rowspan="1">Dataset</th>
    <th rowspan="1">Model</th>
    <th colspan="1">NS. Qualities (MAP)</th>
<th colspan="1">NS. Instrument (ACC)</th>
  </tr>
</thead>

<tbody align="center">
<tr>
    <td rowspan="2">NSynth</td>
    <td>Pengi</td>
    <td>0.3860</td>
    <td>0.5007</td>
  </tr>
<tr>
    <td>Qwen-Audio</td>
    <td><strong>0.4742</strong></td>
    <td><strong>0.7882</strong></td>
  </tr>
</table>

æˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

### é—²èŠèƒ½åŠ›æµ‹è¯„

å—é™äºå­¦æœ¯é¢†åŸŸç¼ºä¹ç³»ç»Ÿæ€§çš„Chatç±»çš„Audioæ¨¡å‹çš„è¯„æµ‹æ–¹æ³•, æˆ‘ä»¬ä¸»è¦æä¾›äº†æ¼”ç¤ºæ¡ˆä¾‹[TUTORIAL](TUTORIAL_zh.md)å’ŒDemoä¾›è°ƒç”¨ã€‚Qwen-Audio-Chatå¯ä»¥è¢«å¹¿æ³›ç”¨äºè¯­éŸ³è¯†åˆ«ï¼Œè¯­éŸ³ç¿»è¯‘ï¼Œç¯å¢ƒéŸ³ç†è§£ï¼Œå¤šéŸ³é¢‘ç†è§£ï¼Œè¯­éŸ³å®šä½ä»¥åŠå¤–éƒ¨è¯­éŸ³ç¼–è¾‘æ¨¡å‹è°ƒç”¨ç­‰åŠŸèƒ½ã€‚


## éƒ¨ç½²è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
* FFmpeg
<br>
## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤– ModelScope å’Œ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨ Qwen-Audio å’Œ Qwen-Audio-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚å…³äºæ›´å¤šç”¨æ³•ï¼Œè¯·å‚è€ƒ[æ•™ç¨‹](TUTORIAL_zh.md)ã€‚ç›®å‰Qwen-Audioä»¥åŠQwen-Audio-Chatæ¨¡å‹å¤„ç†30ç§’ä»¥å†…çš„éŸ³é¢‘è¡¨ç°æ›´ä½³ã€‚

#### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨ Qwen-Audio-Chat è¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ã€‚**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-Audio-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="cuda", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-Audio-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
query = tokenizer.from_list_format([
    {'audio': 'assets/audio/1272-128104-0000.flac'}, # Either a local path or an url
    {'text': 'what does the person say?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# The person says: "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel".

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, 'Find the start time and end time of the word "middle classes"', history=history)
print(response)
# The word "middle classes" starts at <|2.33|> seconds and ends at <|3.26|> seconds.

```



è¿è¡ŒQwen-AudioåŒæ ·éå¸¸ç®€å•ã€‚

<summary>è¿è¡ŒQwen-Audio</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-Audio", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio", device_map="cuda", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-Audio", trust_remote_code=True)
audio_url = "assets/audio/1272-128104-0000.flac"
sp_prompt = "<|startoftranscription|><|en|><|transcribe|><|en|><|notimestamps|><|wo_itn|>"
query = f"<audio>{audio_url}</audio>{sp_prompt}"
audio_info = tokenizer.process_audio(query)
inputs = tokenizer(query, return_tensors='pt', audio_info=audio_info)
inputs = inputs.to(model.device)
pred = model.generate(**inputs, audio_info=audio_info)
response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False,audio_info=audio_info)
print(response)
# <audio>assets/audio/1272-128104-0000.flac</audio><|startoftranscription|><|en|><|transcribe|><|en|><|notimestamps|><|wo_itn|>mister quilting is the apostle of the middle classes and we are glad to welcome his gospel<|endoftext|>
# 
```


è‹¥åœ¨ä½¿ç”¨ä¸Šè¿°ä»£ç æ—¶ç”±äºå„ç§åŸå› æ— æ³•ä» Hugging Face æ‹‰å–æ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼Œå†ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
model_id = 'qwen/Qwen-Audio-Chat'
revision = 'master'
model_dir = snapshot_download(model_id, revision=revision)

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="cuda",
    trust_remote_code=True
).eval()
```

#### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
)
import torch
model_id = 'qwen/Qwen-Audio-Chat'
revision = 'master'

model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
if not hasattr(tokenizer, 'model_dir'):
    tokenizer.model_dir = model_dir
# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()

# ç¬¬ä¸€è½®å¯¹è¯
query = tokenizer.from_list_format([
    {'audio': 'assets/audio/1272-128104-0000.flac'}, # Either a local path or an url
    {'text': 'what does the person say?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# The person says: "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel".

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, 'Find the start time and end time of the word "middle classes"', history=history)
print(response)
# The word "middle classes" starts at <|2.33|> seconds and ends at <|3.26|> seconds.
```

<br>

## Demo

### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```
python web_demo_audio.py
```

<br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜… [FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br>


## å›¢é˜Ÿæ‹›è˜

æˆ‘ä»¬æ˜¯é€šä¹‰åƒé—®è¯­éŸ³å¤šæ¨¡æ€å›¢é˜Ÿï¼Œè‡´åŠ›äºä»¥é€šä¹‰åƒé—®ä¸ºæ ¸å¿ƒï¼Œæ‹“å±•éŸ³é¢‘å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°è‡ªç”±çµæ´»çš„éŸ³é¢‘äº¤äº’ã€‚ç›®å‰å›¢é˜Ÿè“¬å‹ƒå‘å±•ä¸­ï¼Œå¦‚æœ‰æ„å‘å®ä¹ æˆ–å…¨èŒåŠ å…¥æˆ‘ä»¬ï¼Œè¯·å‘é€ç®€å†è‡³qwen_audio@list.alibaba-inc.com.
<br>

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-Audioå’ŒQwen-Audio-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚
<br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚
<br>


## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„è®ºæ–‡å’Œä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘:star: å’Œå¼•ç”¨ :pencil: :)

```BibTeX
@article{Qwen-Audio,
  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},
  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.07919},
  year={2023}
}
```
